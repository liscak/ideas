1. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate natural language to scripting language in games. 
Explanation: you write(or generate) a game script(the logic) in english(or other language) and the transformer based model converts/translate the english text to code that the game will execute.
For example: "The animal moved to the left." would move the animal on screen by one block(or some other unit) to the left(on the screen or map, etc)  
TEXT --> CODE(javascript, luascript, python, etc.) --> Execution.
Text would be used as input for the transformer based model and the output from the model would be the game script and the game script would be executed by the game.
2. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate code from one programming language to another programming language.
For example convert/translate Python code to equivalent Javascript code, where the output and side effects of functions are identical.
3. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate programming language code to binary code(executable) and vice versa(binary code to programming language code)
4. AI RPG where you write or select your actions in english and the game randomly generates a floating point number to indicate the chance of success.  
The random number indicates if the action taken is succesfull or not and the game would alter the story accordingly.  
Text --> ACTION --> RANDOM NUMBER GENERATION --> STORY ALTERATION.  
The game would generate the story with a Transformer based model and present you some choices(generated) of action or you would write your own.
The game would then generate the random number indicating the success represented either in binary ((0, fail), (1, success)) integer/text or more detailed form(float in range between 0 and 1 where the number 1 could represent critical success and 0 critical or deadly failure, 0.5 could represent some form of success 
The sucess rate would be either in integer floating point or text form.  
There could be two ways to implement this:1. Append the random number in number or text form to the action taken and use that as input for the transformer based model and let the transformer generate the outcome. 2. Use the random number in a game system to compute the outcome. 

5. Use IGPT other Transformer based models to generate game images(game props, backgrounds) from the story (text) generated by the transformer based model.  
First, generate the story(text) with a GPT-2 or other Transformer based model.  
Second, use the the text(one or multiple sentences or the whole text) as input into IGPT.  
Third, let IGPT generate the Image.  
Fourth, use the Image as the main Image in the game Window or as a Background?  
Or one could just detect the entities in the story/text and use those(the names and descriptions) as input into IGPT
6. Can GPT-2 or other Transformer based models be used to generate a fake dataset?  
I tried to use GPT-2 with the following input:  
NAME:HENRI,AGE:32,LOCATION:GERMANY;NAME:JAMES,AGE:64,LOCATION:RUSSIA;  
Output from GPT-2:  
NAME:HENRI,AGE:32,LOCATION:GERMANY;NAME:JAMES,AGE:64,LOCATION:RUSSIA;NAME:GARY,AGE:80,LOCATION:FRANCE;  
It looks like GPT-2 can generate a sequence based on a pattern.  
Link: <https://huggingface.co/gpt2?text=NAME%3AHENRI%2CAGE%3A32%2CLOCATION%3AGERMANY%3BNAME%3AJAMES%2CAGE%3A64%2CLOCATION%3ARUSSIA%3B>  
7. Generate Animation Data with GPT-2, GPT-3, BERT or other Transformer based models?  
Use a english description of movement as input into the GPT-2, GPT-3, BERT or other Transformer based model and let it compute the Animation Data.  
Example:  
* Input:
"Move the characters hands up"  
The output would be the animation data(a point cloud, voxels, or some other format)
8. Use GPT-2 or other Transformer based models to generate a video sequence, either by using a single or multiple frames(represented in text form) as input or by using a sentence, description(of an object), or a whole document/story in text form as input.  
It could be possible using the Longformer transformer based model.  
9. Use GPT-2 or other Transformer based models to generate a list of entities(characters with descriptions and stats) for games.  
The system could generate characters with names, descriptions, stats, and more(like relationships of characters, attitudes, moods and more.)  
Input:  
NAME:SANDRA,WEAPONS:KNIFE,MOOD:SAD,STRENGTH:MEELE WEAPONS,WEAKNES:FIRE;
10. Use roberta large or other transformer model for token classfication to find entities in descriptions,stories,text and use those in game.  
NAME:SANDRA,WEAPONS:KNIFE,MOOD:SAD,STRENGTH:MEELE WEAPONS,WEAKNES:FIRE;NAME:JAMES,WEAPONS:SHOTGUN,MOOD:CHEERFUL,STRENGTH:SHORT RANGE WEAPONS,WEAKNES:ICE;  
Link: <https://huggingface.co/xlm-roberta-large-finetuned-conll03-german?text=NAME%3ASANDRA%2CWEAPONS%3AKNIFE%2CMOOD%3ASAD%2CSTRENGTH%3AMEELE+WEAPONS%2C+WEAKNES%3AFIRE%3BNAME%3AJAMES%2CWEAPONS%3ASHOTGUN%2CMOOD%3ACHEERFUL%2CSTRENGTH%3ASHORT+RANGE+WEAPONS%2C+WEAKNES%3AICE%3B>  
The game would know that the story is about those entities, could show them on screen(load/display the sprites,images of those entities) or generate new images,sprites of those entities with a GAN network or IGPT.  
If the token classification network(like roberta,electra, etc) detects entities in text(like in a sentece or multiple sentences of a story, the game would load the images(or sprites) of those(said) entities, like for an example show the entities(characters) side by side to simulate with the text in the middle of ath the bottom of the screen to simulate a dialog.
The token classification network could be use to detect the environment(location) described in the sentence,story,text and show an image of the location on screen(like game screen),
if the location/environment would not be avaiable in memory the system could use similarity search to search for a similar image or generate a new one with GAN or IGPT.
Example:  
Input: "It is the year 1946 in Germany, the city of ... is rebuild. Frank and Thomas are looking a new job."  the three dots could be any name of a city.  
Output(TAGS) could be: YEAR:1946; LOCATION: GERMANY,CITY; CHARACTERS: FRANK,THOMAS;  and the game would load a picture of a German city in the year 1946 on the game screen. It could show images of those characters if avaiable or generate new ones, etc.
11. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate game scripts(like actions) to natural language.  
Example(input could be any programming/scripting language or DSL):  
Input: "ENEMY1 ATTACK WIZZARD, DAMAGE:64, WIZZARD HEALTH:100-64"  
Output: "The enemy attacked the wizzard with his dagger. The wizzard could not evade the attack and took 64 DMG, but he is still alife!"
The game(transformer) could infer from the script that the wizzard is still alife.  
12. Use GPT-2, GPT-3, BERT or other Transformer based models to generate ASCII art/graphics, enter the name or description of an object and let the transformer generate a ASCII text string representing the ASCII art.
13. Use a transformer based AutoEncoder for video compression?  
Transformer Encoder:  
Input: Few Frames in High Resolution  
Output: Embedding (low dimensional)  
Transformer Decoder:  
Input: Embedding (low dimensional)  
Output: Few Frames in High Resolution  
The question is should the input be preprocessed with and convolutional neural network(CNN)?  
14. Use a transformer based model for foto/video upscaling?  
15. Use a transformer based model for audio noise cancelling?  
16. Use GPT-2, GPT-3, BERT or other Transformer based models to generate levels/maps/worlds for games.  
The input would be a description of a place and the transformer would generate a design for a level.  
The output could be a list of names of entities and objects with their respective coordinates(positions) that would be used to generate the level on screen.  
If the game would be a ascii roguelike game, the output could be directly the level itself, every character from the output could represent a part of the level, one character would represent an enemy, another character would represent an item, and another one the game character, a wall, etc.  
If the game would be a 2D or 3D the model output could represent the postion of entities and objects on the level.  
For example(in a 2D game) if the input for the transformer would be: "beach" and the level would be divided into tiles then the output could be: SANDTILE[X=0, Y=10], SEATILE[X=10, Y=30], ROCKTILE[X=20, Y=50] where each tile represents an object or entity and the values 'x' and 'y' the coordinates.  
A 3D game entity or object would have 3 coordinates -> x, y and z.  
The transformer could generate a minecraft style map, the output of the transformer would be the names and positions of entities and objects represented by voxels(or even by point clouds).  
17. Use GPT-2, GPT-3, BERT or other Transformer based models to detect events(actions), objects and entities in sentences, story, text and then try to call the event(name) as a function first by looking for the function in a list/array of functions or in a dictionary(hash table) of functions, if the function is found, then call it with the object and entities names as parameters/arguments.  
First detect the event(action), that is the function name. Then detect the objects and entities, now we  have the names of objects and entities. Call the function with the names of objects and entities as arguments/parameters.
Event(event name) ==> function name
Objects/Entities ==> function parameters.  
event(object_name, ..., entity_name, ...)  
18. Use Neural Video Compression for Game Streaming/Cloud Gaming. Either use and Auto-Encoder convolutional neural network to compress the stream and send out embeddings.  
Other ideas for Cloud Gaming/Game Stream video compression:  
Train convolutional autoencoder that takes as input an frame or frames and the output would be the original frame, frames or the same frame, frames in higher resolution. The server would use the encoder to encode the game stream frame or frames into an embedding in realtime and the decoder(from the convolutional autoencoder) on the client device would decode the embedding and create the frame, frames and the software would display the on the screen. Another way to do this would be to use a second network for meta learning the weights of the autoencoder or of a superresolution convolutional neural network and send those weights to the client device and the client would load the model with those weights.  
Game Stream frame,frames --> MetaLearning Convolutional Neural Network   
The metalearning CNN (CNN = convolutional neural network) would take a frame or multiple frames as input and the output would be the weights of a CNN that takes as input an frame, frames in lower resolution and creates a frame, frames in higher resolution(that would be a superresolution network) or the CNN would take as input an embedding and recreate the frame, frames from the embedding.  The server would use the metalerning network to create the weights from the game stream frame,frames and send the weights and low resolution frame,frames(created from the original frame, frames) to the client device. The client devices woudl load the CNN model withs those weights and the CNN would take as input the low resolution frame,frames and the low resolution frame,frames would be transformed by the CNN into higher resolution frames,frames.
