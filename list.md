1. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate natural language to scripting language in games. 
Explanation: you write(or generate) a game script(the logic) in english(or other language) and the transformer based model converts/translate the english text to code that the game will execute.
For example: "The animal moved to the left." would move the animal on screen by one block(or some other unit) to the left(on the screen or map, etc)  
TEXT --> CODE(javascript, luascript, python, etc.) --> Execution.
Text would be used as input for the transformer based model and the output from the model would be the game script and the game script would be executed by the game.
2. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate code from one programming language to another programming language.
For example convert/translate Python code to equivalent Javascript code, where the output and side effects of functions are identical.
3. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate programming language code to binary code(executable) and vice versa(binary code to programming language code)
4. AI RPG where you write or select your actions in english and the game randomly generates a floating point number to indicate the chance of success.  
The random number indicates if the action taken is succesfull or not and the game would alter the story accordingly.  
Text --> ACTION --> RANDOM NUMBER GENERATION --> STORY ALTERATION.  
The game would generate the story with a Transformer based model and present you some choices(generated) of action or you would write your own.
The game would then generate the random number indicating the success represented either in binary ((0, fail), (1, success)) integer/text or more detailed form(float in range between 0 and 1 where the number 1 could represent critical success and 0 critical or deadly failure, 0.5 could represent some form of success 
The sucess rate would be either in integer floating point or text form.  
There could be two ways to implement this:1. Append the random number in number or text form to the action taken and use that as input for the transformer based model and let the transformer generate the outcome. 2. Use the random number in a game system to compute the outcome. 

5. Use IGPT other Transformer based models to generate game images(game props, backgrounds) from the story (text) generated by the transformer based model.  
First, generate the story(text) with a GPT-2 or other Transformer based model.  
Second, use the the text(one or multiple sentences or the whole text) as input into IGPT.  
Third, let IGPT generate the Image.  
Fourth, use the Image as the main Image in the game Window or as a Background?  
Or one could just detect the entities in the story/text and use those(the names and descriptions) as input into IGPT
6. Can GPT-2 or other Transformer based models be used to generate a fake dataset?  
I tried to use GPT-2 with the following input:  
NAME:HENRI,AGE:32,LOCATION:GERMANY;NAME:JAMES,AGE:64,LOCATION:RUSSIA;  
Output from GPT-2:  
NAME:HENRI,AGE:32,LOCATION:GERMANY;NAME:JAMES,AGE:64,LOCATION:RUSSIA;NAME:GARY,AGE:80,LOCATION:FRANCE;  
It looks like GPT-2 can generate a sequence based on a pattern.  
Link: <https://huggingface.co/gpt2?text=NAME%3AHENRI%2CAGE%3A32%2CLOCATION%3AGERMANY%3BNAME%3AJAMES%2CAGE%3A64%2CLOCATION%3ARUSSIA%3B>  
7. Generate Animation Data with GPT-2, GPT-3, BERT or other Transformer based models?  
Use a english description of movement as input into the GPT-2, GPT-3, BERT or other Transformer based model and let it compute the Animation Data.  
Example:  
* Input:
"Move the characters hands up"  
The output would be the animation data(a point cloud, voxels, or some other format)
8. Use GPT-2 or other Transformer based models to generate a video sequence, either by using a single or multiple frames(represented in text form) as input or by using a sentence, description(of an object), or a whole document/story in text form as input.  
It could be possible using the Longformer transformer based model.  
9. Use GPT-2 or other Transformer based models to generate a list of entities(characters with descriptions and stats) for games.  
The system could generate characters with names, descriptions, stats, and more(like relationships of characters, attitudes, moods and more.)  
Input:  
NAME:SANDRA,WEAPONS:KNIFE,MOOD:SAD,STRENGTH:MEELE WEAPONS,WEAKNES:FIRE;
10. Use roberta large or other transformer model for token classfication to find entities in descriptions,stories,text and use those in game.  
NAME:SANDRA,WEAPONS:KNIFE,MOOD:SAD,STRENGTH:MEELE WEAPONS,WEAKNES:FIRE;NAME:JAMES,WEAPONS:SHOTGUN,MOOD:CHEERFUL,STRENGTH:SHORT RANGE WEAPONS,WEAKNES:ICE;  
Link: <https://huggingface.co/xlm-roberta-large-finetuned-conll03-german?text=NAME%3ASANDRA%2CWEAPONS%3AKNIFE%2CMOOD%3ASAD%2CSTRENGTH%3AMEELE+WEAPONS%2C+WEAKNES%3AFIRE%3BNAME%3AJAMES%2CWEAPONS%3ASHOTGUN%2CMOOD%3ACHEERFUL%2CSTRENGTH%3ASHORT+RANGE+WEAPONS%2C+WEAKNES%3AICE%3B>  
The game would know that the story is about those entities, could show them on screen(load/display the sprites,images of those entities) or generate new images,sprites of those entities with a GAN network or IGPT.  
If the token classification network(like roberta,electra, etc) detects entities in text(like in a sentece or multiple sentences of a story, the game would load the images(or sprites) of those(said) entities, like for an example show the entities(characters) side by side to simulate with the text in the middle of ath the bottom of the screen to simulate a dialog.
The token classification network could be use to detect the environment(location) described in the sentence,story,text and show an image of the location on screen(like game screen),
if the location/environment would not be avaiable in memory the system could use similarity search to search for a similar image or generate a new one with GAN or IGPT.
Example:  
Input: "It is the year 1946 in Germany, the city of ... is rebuild. Frank and Thomas are looking a new job."  the three dots could be any name of a city.  
Output(TAGS) could be: YEAR:1946; LOCATION: GERMANY,CITY; CHARACTERS: FRANK,THOMAS;  and the game would load a picture of a German city in the year 1946 on the game screen. It could show images of those characters if avaiable or generate new ones, etc.
11. Use GPT-2, GPT-3, BERT or other Transformer based models to convert/translate game scripts(like actions) to natural language.  
Example(input could be any programming/scripting language or DSL):  
Input: "ENEMY1 ATTACK WIZZARD, DAMAGE:64, WIZZARD HEALTH:100-64"  
Output: "The enemy attacked the wizzard with his dagger. The wizzard could not evade the attack and took 64 DMG, but he is still alife!"
The game(transformer) could infer from the script that the wizzard is still alife.  
12. Use GPT-2, GPT-3, BERT or other Transformer based models to generate ASCII art/graphics, enter the name or description of an object and let the transformer generate a ASCII text string representing the ASCII art.
13. Use a transformer based AutoEncoder for video compression?  
Transformer Encoder:  
Input: Few Frames in High Resolution  
Output: Embedding (low dimensional)  
Transformer Decoder:  
Input: Embedding (low dimensional)  
Output: Few Frames in High Resolution  
The question is should the input be preprocessed with and convolutional neural network(CNN)?  
14. Use a transformer based model for foto/video upscaling?  
15. Use a transformer based model for audio noise cancelling?  
16. Use GPT-2, GPT-3, BERT or other Transformer based models to generate levels/maps/worlds for games.  
The input would be a description of a place and the transformer would generate a design for a level.  
The output could be a list of names of entities and objects with their respective coordinates(positions) that would be used to generate the level on screen.  
If the game would be a ascii roguelike game, the output could be directly the level itself, every character from the output could represent a part of the level, one character would represent an enemy, another character would represent an item, and another one the game character, a wall, etc.  
If the game would be a 2D or 3D the model output could represent the postion of entities and objects on the level.  
For example(in a 2D game) if the input for the transformer would be: "beach" and the level would be divided into tiles then the output could be: SANDTILE[X=0, Y=10], SEATILE[X=10, Y=30], ROCKTILE[X=20, Y=50] where each tile represents an object or entity and the values 'x' and 'y' the coordinates.  
A 3D game entity or object would have 3 coordinates -> x, y and z.  
The transformer could generate a minecraft style map, the output of the transformer would be the names and positions of entities and objects represented by voxels(or even by point clouds).  
17. Use GPT-2, GPT-3, BERT or other Transformer based models to detect events(actions), objects and entities in sentences, story, text and then try to call the event(name) as a function first by looking for the function in a list/array of functions or in a dictionary(hash table) of functions, if the function is found, then call it with the object and entities names as parameters/arguments.  
First detect the event(action), that is the function name. Then detect the objects and entities, now we  have the names of objects and entities. Call the function with the names of objects and entities as arguments/parameters.
Event(event name) ==> function name
Objects/Entities ==> function parameters.  
event(object_name, ..., entity_name, ...)  
18. Use Neural Video Compression for Game Streaming/Cloud Gaming. Either use and Auto-Encoder convolutional neural network to compress the stream and send out embeddings.  
Other ideas for Cloud Gaming/Game Stream video compression:  
Train convolutional autoencoder that takes as input an frame or frames and the output would be the original frame, frames or the same frame, frames in higher resolution. The server would use the encoder to encode the game stream frame or frames into an embedding in realtime and the decoder(from the convolutional autoencoder) on the client device would decode the embedding and create the frame, frames and the software would display the on the screen.  
Another way to do this would be to use a second network for meta learning the weights of the autoencoder or of a superresolution convolutional neural network and send those weights to the client device and the client would load the model with those weights.  
Game Stream frame,frames --> MetaLearning Convolutional Neural Network   
The metalearning CNN (CNN = convolutional neural network) would take a frame or multiple frames as input and the output would be the weights of a CNN that takes as input an frame, frames in lower resolution and creates a frame, frames in higher resolution(that would be a superresolution network) or the CNN would take as input an embedding and recreate the frame, frames from the embedding.  The server would use the metalerning network to create the weights from the game stream frame,frames and send the weights and low resolution frame,frames(created from the original frame, frames) to the client device. The client devices woudl load the CNN model withs those weights and the CNN would take as input the low resolution frame,frames and the low resolution frame,frames would be transformed by the CNN into higher resolution frames,frames.  
19. All transformer based models here mentioned would be trained either directly(with input output pairs) or in a one-shot or few-shot manner: by giving it examples as part of the input. (like openai mentioned in: https://arxiv.org/abs/2005.14165)  
20. Use continuous signed distance functions/ DeepSDF for game textures.  
Train an multilayer  perceptron(MLP) that outputs a RGB value for a (x,y) pixel.  
The input are the x and y coordiantes represented as floating point values between 0 and 1, both inclusive.
The output is the RGB value of the pixel, so 3 channels, an array of 3(three) values.
The game texture is compressed/represented as the weights of the neural network.  
The game texture is compressed(learned) beforehand.  
The original game texture is an image.  
For every pixel in the texture(or image) we get its coordinates and RGB value, we normalize the coordinate(x and y pair) as float values between 0 an 1, use  them as the neural network input(feed them into the network) and the output are is RGB value of the pixel at that coordinate. So the output is a array of 3 values (because we have 3 channels): Red Green and Blue.  The ground truth value(the pixel value of the original texture) is used in the optimizer to compute the mean squared error(or other type of erro signal, like MSA, etc).  After training and overfiting the neural network with the one texture represented as the dataset we recover the texture(or just concrete pixels of the texture) by looping over the coordinates(all of the x and y pairs).  The weights of the trained/overfited network represent on texture.  
The texture will be recovered when the game loads the texture.  
21. One-Shot SDF learing mesh geometry?  
22. Use NN trained with SDF for long term storage, use neural radiance field to learn an object and use that network(that learned the object or enironment) as long term memory.  
23. Use CNN on neural radiance field generated volumes or images for find object or for semantic segmentation.  
24. Saving a video/animation into a MLP(multi layer perceptron) neural network, where the input is the x and y coordinate in the floating point form(between 0 and 1 inclusive) for one pixel and a floating point value representing the continuous timestep/time, that is 0 represents the start(first frame) of the video/animation, 0.25 would represent the frame at the one fourth timestep/time of the full length of the video/animation ,0.5 the middle(frame) of the video/animation and 1 the end(last frame) of the vide/animation, other values represent their respective timesteps. And the output would be the one pixel RGB value(3 values: red green and blue all normalized and represented as floating point values between 0 and 1 both inclusive) at the queried timestep/time. This could be used to compress video files, video streams, game streams(game streaming), etc.  
Example: A video(file) would be 50 minutes long, that would be 3000 seconds(50 * 60), the first frame would be at time position 0( zero seconds) and represented as 0 float,  
the one fourth time position of the whole video/animation would be 750 seconds(3000 * 0.25) represented as 0.25 float.  
25. Use GPT-2, GPT-3, BERT or other Transformer based models to generate textures, shaders or meshes.  
A transformer model would be used to generate mesh data, or texture data or a shader.  
The input would be a description of the texture, shader or mesh and the output would be either the data: either the texture, shader or mesh.  
In games(ingame) a user would write a description of an objest/mesh/texture/shader and the game would send the query/text-string to an Transformer(or an API/transformer running on a server), the query would consist of either just the description or of two or more examples(description and data pairs) and the description the user wrote. The server/transformer would return the output/data(texture,shader or mesh). The game would load and display the data(texture,shader or mesh). So the system would generate new data based on the descriptions on the fly.  
26. A social/party online game where multiple people need to complete objectives in the game world by describing/naming objects that would spawn into the game world and could be used by the users/players to complete the objectives in creative ways. The descriptions/names of the objects would be send to a Transformer based model and that model would generate the object(game model[either a 2D sprite, 3D object with mesh, etc.]) data. The query could consist of either the description/name of the object or of two or more examples(description/name and data pairs) and the description/name itself. The data would be loaded into the game and the game would display the object. The object would have attached code or properties generated by the Transformer model that make the object interactive.  
27. A social/party online game where multiple people need to complete objectives in the game world by describing/naming objects word by word that would spawn into the game world and could be used by the users to complete the objectives in creative ways.  
All users/players would describe one(single) object to be spawned, each user would write one(single) word or each user would write a part of the description or of the name and all parts would be combined together as one(final) description or name of an object.
The descriptions/names of the objects would be send to a Transformer based model and that model would generate the object(game model[either a 2D sprite, 3D object with mesh, etc.]) data. The query could consist of either the description/name of the object or of two or more examples(description/name and data pairs) and the description/name itself. The data would be loaded into the game and the game would display the object. The object would have attached code or properties generated by the Transformer model that make the object interactive.  
28. A social/party online game where multiple people(the first group) need to gues what object was spawned/displayed in the game world by the other players(second group).  The second group describes/names objects word by word that would be spawn into the game world and the first group needs to gues the name of the object/objects that the second grounp spawned.  
All second group would describe one(single) object to be spawned, each user(of the second group) would write one(single) word or each user(of the second group) would write a part of the description or of the name and all parts would be combined together as one(final) description or name of an object.
The descriptions/names of the objects would be send to a Transformer based model and that model would generate the object(game model[either a 2D sprite, 3D object with mesh, etc.]) data. The query could consist of either the description/name of the object or of two or more examples(description/name and data pairs) and the description/name itself. The data would be loaded into the game and the game would display the object. The object would have attached code or properties generated by the Transformer model that make the object interactive.  
29. Use OpenNARS(https://github.com/opennars/opennars) with GPT-3(or a newer GPT version), where GPT-3 would be used to convert/translate natural language to NAL and NAL to natural language. Provide GPT-3 with english to NAL pairs and NAL to english pairs for few shot learning(learning without training). [For english to NAL]The input for GPT-3 would be english-NAL pairs and the query in english and GPT-3 would output NAL. And for NAL to english the input for GPT-3 would be NAL-english pairs and the query in NAL and GPT-3 would output english(text).  
(Or train a transformer model directly on NAL-english pairs or english-NAL pairs where the first thing is the input and the second is the desired output) 
30. Differentiable NARS/OpenNARS. It could be written in python with the pytorch library and would/could be GPU/TPU accelerated. It could be trained(with backpropagation/autodiff) to learn new rules. The differentiable NARS/OpenNARS could be combined with other models(neural networks) to build an end to end trainable model/system.
