1. Use GPT-2, GPT-3, BERT or other Transformer based models to convert natural language to scripting language in games. 
Explanation: you write(or generate) a game script(the logic) in english(or other language) and the transformer based model converts the english text to code that the game will execute.
For example: "The animal moved to the left." would move the animal on screen by one block(or some other unit) to the left(on the screen or map, etc)  
TEXT --> CODE(javascript, luascript, python, etc.) --> Execution.
Text would be used as input for the transformer based model and the output from the model would be the game script and the game script would be executed by the game.
2. Use GPT-2, GPT-3, BERT or other Transformer based models to convert code from one programming language to another programming language.
For example convert Python code to equivalent Javascript code, where the output and side effects of functions are identical.
3. Use GPT-2, GPT-3, BERT or other Transformer based models to convert programming language code to binary code(executable) and vice versa(binary code to programming language code)
4. AI RPG where you write or select your actions in english and the game randomly generates a floating point number to indicate the chance of success.  
The random number indicates if the action taken is succesfull or not and the game would alter the story accordingly.  
Text --> ACTION --> RANDOM NUMBER GENERATION --> STORY ALTERATION.  
The game would generate the story with a Transformer based model and present you some choices(generated) of action or you would write your own.
The game would then generate the random number indicating the success represented either in binary ((0, fail), (1, success)) integer/text or more detailed form(float in range between 0 and 1 where the number 1 could represent critical success and 0 critical or deadly failure, 0.5 could represent some form of success 
The sucess rate would be either in integer floating point or text form.  
There could be two ways to implement this:1. Append the random number in number or text form to the action taken and use that as input for the transformer based model and let the transformer generate the outcome. 2. Use the random number in a game system to compute the outcome. 

5. Use IGPT other Transformer based models to generate game images(game props, backgrounds) from the story (text) generated by the transformer based model.  
First, generate the story(text) with a GPT-2 or other Transformer based model.  
Second, use the the text(one or multiple sentences or the whole text) as input into IGPT.  
Third, let IGPT generate the Image.  
Fourth, use the Image as the main Image in the game Window or as a Background?  
Or one could just detect the entities in the story/text and use those(the names and descriptions) as input into IGPT
6. Can GPT-2 or other Transformer based models be used to generate a fake dataset?  
I tried to use GPT-2 with the following input:  
NAME:HENRI,AGE:32,LOCATION:GERMANY;NAME:JAMES,AGE:64,LOCATION:RUSSIA;  
Output from GPT-2:  
NAME:HENRI,AGE:32,LOCATION:GERMANY;NAME:JAMES,AGE:64,LOCATION:RUSSIA;NAME:GARY,AGE:80,LOCATION:FRANCE;  
It looks like GPT-2 can generate a sequence based on a pattern.  
Link: <https://huggingface.co/gpt2?text=NAME%3AHENRI%2CAGE%3A32%2CLOCATION%3AGERMANY%3BNAME%3AJAMES%2CAGE%3A64%2CLOCATION%3ARUSSIA%3B>  
7. Generate Animation Data with GPT-2, GPT-3, BERT or other Transformer based models?  
Use a english description of movement as input into the GPT-2, GPT-3, BERT or other Transformer based model and let it compute the Animation Data.  
Example:  
* Input:
"Move the characters hands up"  
The output would be the animation data(a point cloud, voxels, or some other format)
8. Use GPT-2 or other Transformer based models to generate a video sequence, either by using a single or multiple frames(represented in text form) as input or by using a sentence, description(of an object), or a whole document/story in text form as input.  
It could be possible using the Longformer transformer based model.  
9. Use GPT-2 or other Transformer based models to generate a list of entities(characters with descriptions and stats) for games.  
The system could generate characters with names, descriptions, stats, and more(like relationships of characters, attitudes, moods and more.)  
Input:  
NAME:SANDRA,WEAPONS:KNIFE,MOOD:SAD,STRENGTH:MEELE WEAPONS,WEAKNES:FIRE;
10. Use roberta large token classfication to find entities in descriptions,stories,text and use those in game.  
NAME:SANDRA,WEAPONS:KNIFE,MOOD:SAD,STRENGTH:MEELE WEAPONS,WEAKNES:FIRE;NAME:JAMES,WEAPONS:SHOTGUN,MOOD:CHEERFUL,STRENGTH:SHORT RANGE WEAPONS,WEAKNES:ICE;  
Link: <https://huggingface.co/xlm-roberta-large-finetuned-conll03-german?text=NAME%3ASANDRA%2CWEAPONS%3AKNIFE%2CMOOD%3ASAD%2CSTRENGTH%3AMEELE+WEAPONS%2C+WEAKNES%3AFIRE%3BNAME%3AJAMES%2CWEAPONS%3ASHOTGUN%2CMOOD%3ACHEERFUL%2CSTRENGTH%3ASHORT+RANGE+WEAPONS%2C+WEAKNES%3AICE%3B>  
The game would know that the story is about those entities, could show them on screen(load/display the sprites,images of those entities) or generate new images,sprites of those entities with a GAN network or IGPT.  
If the token classification network(like roberta,electra, etc) detects entities in text(like in a sentece or multiple sentences of a story, the game would load the images(or sprites) of those(said) entities, like for an example show them(characters) side by side to simulate with the text in the middle of ath the bottom of the screen to simulate a dialog.
The token classification network could be use to detect the environment(location) described in the sentence,story,text and show an image of the location on screen(like game screen),
if the location/environment would not be avaiable in memory the system could use similarity search to search for a similar image or generate a new one with GAN or IGPT.
Example:  
Input: "It is the year 1946 in Germany, the city of ... is rebuild. Frank and Thomas are looking a new job."  the three dots could be any name of a city.  
Output(TAGS) could be: YEAR:1946; LOCATION: GERMANY,CITY; CHARACTERS: FRANK,THOMAS;  and the game would load a picture of a German city in the year 1946 on the game screen. It could show images of those characters if avaiable or generate new ones.
