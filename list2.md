1. Break up an image(with a known class) into many smaller tiles(each tile would be shifted from the previous tile by a certain amount of pixels(defined by the stride value.)
Save every tile into the faiss index(or list) and every tiles class into a list or database.
Repeat this for every image in a dataset.
When classifying an image break up the image into many smaller tiles(where every tile is shifted from the previous).
Take the tile first tile from the image and similarity search for the most similar tile in the faiss index. When the most similar tile is found save the (found) tiles known class and similarity value into a list if the similarity value is less zero or less than a treshold(hyper-parameter) value then either the similarity value will be set to zero or the tile/patch wont have a class, if a tile has/belongs to multiple classes then get either all the classes it belongs to or just a few, the tile will have multiple similarity values for each class it belongs to. (tldr. we will classify every tile of the image). Repeat for every tile in the image(until the last tile is classified). We ge an array/list of regions which have a class or multiple classes.(a list(or image) where every region is classified).
Now for each found class in the list we summ up its similarity values, then the class with the highest value represent the class our image belongs to(We can now tell to which class the whole image belongs). And we know which part of the image belongs to a certain class or classes.

2. Convert multiple images or a video of an object to a textured mesh using image segmentation and differential silhouette rendering (https://pytorch3d.org/tutorials/fit_textured_mesh).
   First take multiple images of an object or use a video separated into frame/images. Then using pixelib (https://github.com/ayoolaolafenwa/PixelLib) detect and segmentate the object on the images, thus creating the silhouettes need for fiting the mesh using differential silhouette rendering. Then using the silhouettes, fit the mesh with differential silhouette rendering  and its textures using differential textured rendering. The questions is how to detect the camera position of each image/silhouette? If we take images using a phone we can record/save the phone position(using values from accelerometer and other sensors) while taking the image or we can compute the approximate camera positions from the images themselves.  
3. Traing an Convolutional neural network or Transformer to generate textured meshes from images. The input of the network/model would be multiple images and the output would be the textured mesh.
